import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json

def prepare_nlp_features(df):
    """
    PrÃ©pare les features texte pour l'entraÃ®nement NLP
    """
    print("=" * 60)
    print("Ã‰TAPE 4: PRÃ‰PARATION DES FEATURES NLP")
    print("=" * 60)
    
    print("ğŸ“ CrÃ©ation des features texte...")
    
    # CrÃ©er des features texte combinÃ©es pour DistilBERT
    text_features = []
    
    for idx, row in df.iterrows():
        text_parts = []
        
        # Features principales pour la dÃ©tection d'attaques
        if 'proto' in df.columns:
            text_parts.append(f"protocol_{int(row['proto'])}")
        if 'service' in df.columns:
            text_parts.append(f"service_{int(row['service'])}")
        if 'state' in df.columns:
            text_parts.append(f"state_{int(row['state'])}")
        if 'srcip' in df.columns:
            # Extraire seulement le premier octet de l'IP pour Ã©viter le bruit
            try:
                first_octet = str(row['srcip']).split('.')[0]
                text_parts.append(f"srcip_{first_octet}")
            except:
                pass
        if 'dstip' in df.columns:
            try:
                first_octet = str(row['dstip']).split('.')[0]
                text_parts.append(f"dstip_{first_octet}")
            except:
                pass
        
        # Ajouter des informations de trafic
        if 'sbytes' in df.columns and row['sbytes'] > 0:
            text_parts.append("has_sent_bytes")
        if 'dbytes' in df.columns and row['dbytes'] > 0:
            text_parts.append("has_received_bytes")
        if 'dur' in df.columns and row['dur'] > 1.0:
            text_parts.append("long_duration")
        
        text_features.append(" ".join(text_parts))
    
    df['text_features'] = text_features
    
    print(f"âœ… Features texte crÃ©Ã©es. Exemple:")
    print(f"   '{text_features[0][:80]}...'")
    print(f"   Longueur moyenne: {np.mean([len(text) for text in text_features]):.0f} caractÃ¨res")
    
    return df

def prepare_train_test_split(df):
    """
    PrÃ©pare la division train/validation/test
    """
    print("\nğŸ¯ PrÃ©paration des splits train/validation/test...")
    
    # VÃ©rifier la distribution des labels
    label_dist = df['Label'].value_counts()
    print(f"Distribution des labels: {label_dist.to_dict()}")
    
    # Division stratifiÃ©e
    train_df, temp_df = train_test_split(
        df, 
        test_size=0.3, 
        random_state=42, 
        stratify=df['Label']
    )
    
    val_df, test_df = train_test_split(
        temp_df, 
        test_size=0.5, 
        random_state=42, 
        stratify=temp_df['Label']
    )
    
    print(f"âœ… Division terminÃ©e:")
    print(f"   Train:      {len(train_df)} Ã©chantillons")
    print(f"   Validation: {len(val_df)} Ã©chantillons")
    print(f"   Test:       {len(test_df)} Ã©chantillons")
    
    # VÃ©rifier la distribution dans chaque split
    print(f"\nğŸ“Š Distribution dans chaque split:")
    for split_name, split_df in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:
        dist = split_df['Label'].value_counts()
        print(f"   {split_name:12} - Normal: {dist[0]:>5}, Attack: {dist[1]:>5}")
    
    return train_df, val_df, test_df

def save_nlp_data(train_df, val_df, test_df):
    """
    Sauvegarde les donnÃ©es prÃ©parÃ©es pour NLP
    """
    print("\nğŸ’¾ Sauvegarde des donnÃ©es NLP...")
    
    # Sauvegarder les splits
    train_df[['text_features', 'Label']].to_csv('./data/nlp_train.csv', index=False)
    val_df[['text_features', 'Label']].to_csv('./data/nlp_val.csv', index=False)
    test_df[['text_features', 'Label']].to_csv('./data/nlp_test.csv', index=False)
    
    print("âœ… DonnÃ©es NLP sauvegardÃ©es:")
    print(f"   nlp_train.csv: {len(train_df)} Ã©chantillons")
    print(f"   nlp_val.csv:   {len(val_df)} Ã©chantillons")
    print(f"   nlp_test.csv:  {len(test_df)} Ã©chantillons")
    
    # Sauvegarder les mÃ©tadonnÃ©es
    metadata = {
        'total_samples': len(train_df) + len(val_df) + len(test_df),
        'train_samples': len(train_df),
        'val_samples': len(val_df),
        'test_samples': len(test_df),
        'class_distribution': {
            'train': train_df['Label'].value_counts().to_dict(),
            'val': val_df['Label'].value_counts().to_dict(),
            'test': test_df['Label'].value_counts().to_dict()
        },
        'text_feature_stats': {
            'average_length': np.mean([len(text) for text in train_df['text_features']]),
            'vocabulary_size': len(set(' '.join(train_df['text_features']).split()))
        }
    }
    
    with open('./results/nlp_preparation_report.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("ğŸ“Š MÃ©tadonnÃ©es NLP sauvegardÃ©es")

def main():
    """
    EXÃ‰CUTION PRINCIPALE - PRÃ‰PARATION NLP
    """
    # Charger les donnÃ©es Ã©quilibrÃ©es
    print("ğŸ“¥ Chargement des donnÃ©es Ã©quilibrÃ©es...")
    df = pd.read_csv('./data/UNSW-NB15_undersampled.csv')
    print(f"ğŸ“Š Dataset Ã©quilibrÃ©: {df.shape}")
    
    # PrÃ©parer les features NLP
    df_nlp = prepare_nlp_features(df)
    
    # PrÃ©parer les splits
    train_df, val_df, test_df = prepare_train_test_split(df_nlp)
    
    # Sauvegarder les donnÃ©es NLP
    save_nlp_data(train_df, val_df, test_df)
    
    # Sauvegarder le dataset complet NLP-ready
    df_nlp.to_csv('./data/UNSW-NB15_nlp_ready.csv', index=False)
    print("ğŸ’¾ Dataset NLP-ready sauvegardÃ©: UNSW-NB15_nlp_ready.csv")
    
    print(f"\n{'âœ…'*20}")
    print("Ã‰TAPE 4 TERMINÃ‰E AVEC SUCCÃˆS!")
    print("ğŸ¤– DonnÃ©es prÃªtes pour l'entraÃ®nement DistilBERT!")
    print(f"{'âœ…'*20}")

if __name__ == "__main__":
    main()