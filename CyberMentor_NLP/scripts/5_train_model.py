import pandas as pd
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from torch.optim import AdamW
from torch.utils.data import DataLoader
import os
import time

def train_fast():
    """Version RAPIDE de l'entra√Ænement - 5-10 minutes max"""
    print("üöÄ ENTRA√éNEMENT RAPIDE DISTILBERT")
    print("‚è±Ô∏è  Dur√©e estim√©e: 5-10 minutes")
    
    # Configuration optimis√©e
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è  Device: {device}")
    
    # Charger UNIQUEMENT un sous-√©chantillon pour aller vite
    print("üì• Chargement des donn√©es (sous-√©chantillon)...")
    train_df = pd.read_csv('./data/nlp_train.csv')
    
    # Prendre seulement 2000 √©chantillons pour aller vite
    sample_size = 2000
    train_df = train_df.sample(n=sample_size, random_state=42)
    
    print(f"üìä Utilisation de {sample_size} √©chantillons pour entra√Ænement rapide")
    
    # Tokenizer
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    # Pr√©parer les donn√©es rapidement
    def prepare_fast_data(texts, labels):
        print("üî§ Tokenisation rapide...")
        encodings = tokenizer(
            texts, 
            truncation=True, 
            padding=True, 
            max_length=64,  # Longueur r√©duite
            return_tensors='pt'
        )
        return {
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask'],
            'labels': torch.tensor(labels)
        }
    
    # Pr√©parer les donn√©es
    train_texts = train_df['text_features'].tolist()[:sample_size]
    train_labels = train_df['Label'].tolist()[:sample_size]
    
    train_data = prepare_fast_data(train_texts, train_labels)
    
    # Dataset simple
    class FastDataset(torch.utils.data.Dataset):
        def __init__(self, encodings):
            self.encodings = encodings
            
        def __getitem__(self, idx):
            return {
                'input_ids': self.encodings['input_ids'][idx],
                'attention_mask': self.encodings['attention_mask'][idx],
                'labels': self.encodings['labels'][idx]
            }
            
        def __len__(self):
            return len(self.encodings['labels'])
    
    train_dataset = FastDataset(train_data)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Batch plus grand
    
    # Mod√®le
    print("ü§ñ Chargement du mod√®le...")
    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)
    model.to(device)
    model.train()
    
    # Optimizer
    optimizer = AdamW(model.parameters(), lr=2e-5)  # Learning rate plus √©lev√©
    
    # Entra√Ænement RAPIDE - 2 epochs seulement
    print("üéØ D√©but entra√Ænement RAPIDE (2 epochs)...")
    start_time = time.time()
    
    for epoch in range(2):  # SEULEMENT 2 EPOCHS
        epoch_start = time.time()
        total_loss = 0
        model.train()
        
        for batch_idx, batch in enumerate(train_loader):
            optimizer.zero_grad()
            
            inputs = {
                'input_ids': batch['input_ids'].to(device),
                'attention_mask': batch['attention_mask'].to(device),
                'labels': batch['labels'].to(device)
            }
            
            outputs = model(**inputs)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # Afficher la progression
            if batch_idx % 10 == 0:
                elapsed = time.time() - epoch_start
                batches_done = batch_idx + 1
                total_batches = len(train_loader)
                progress = (batches_done / total_batches) * 100
                
                print(f"  Epoch {epoch+1}: {progress:.1f}% ({batches_done}/{total_batches}), "
                      f"Loss: {loss.item():.4f}, Temps: {elapsed:.0f}s")
        
        avg_loss = total_loss / len(train_loader)
        epoch_time = time.time() - epoch_start
        print(f"‚úÖ Epoch {epoch+1}/2 termin√©e - Loss: {avg_loss:.4f}, Temps: {epoch_time:.0f}s")
    
    total_time = time.time() - start_time
    print(f"‚è±Ô∏è  Temps total d'entra√Ænement: {total_time:.0f} secondes")
    
    # Sauvegarde rapide
    print("üíæ Sauvegarde du mod√®le...")
    model.save_pretrained('./models/cybermentor_nlp_model_fast')
    tokenizer.save_pretrained('./models/cybermentor_nlp_model_fast')
    
    print("üéâ ENTRA√éNEMENT RAPIDE TERMIN√â!")
    return model

def main():
    """
    ENTRA√éNEMENT RAPIDE - 5-10 MINUTES MAX
    """
    print("=" * 60)
    print("√âTAPE 5: ENTRA√éNEMENT RAPIDE DISTILBERT")
    print("=" * 60)
    
    # Cr√©er les dossiers
    os.makedirs('./models', exist_ok=True)
    
    try:
        model = train_fast()
        
        print(f"\n{'üéâ'*20}")
        print("√âTAPE 5 TERMIN√âE AVEC SUCC√àS!")
        print("ü§ñ Mod√®le DistilBERT entra√Æn√© RAPIDEMENT!")
        print("üìÅ Mod√®le sauvegard√© dans: ./models/cybermentor_nlp_model_fast/")
        print("‚è±Ô∏è  Pr√™t pour l'√©valuation!")
        print(f"{'üéâ'*20}")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        print("üí° Essayez avec encore moins de donn√©es...")

if __name__ == "__main__":
    main()